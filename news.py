import requests
import json
from datetime import datetime
import time
import random


class WeiboHotSearchCrawler:
    def __init__(self):
        # å¾®åšçƒ­æœAPIåœ°å€
        self.url = "https://weibo.com/ajax/side/hotSearch"

        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Cookie': ''  # éœ€è¦å¡«å…¥æœ‰æ•ˆçš„Cookie
        }

    def get_random_user_agent(self):
        """éšæœºç”ŸæˆUser-Agent"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        return random.choice(user_agents)

    def fetch_hot_search(self):
        """è·å–çƒ­æœæ•°æ®"""
        try:
            # éšæœºæ›´æ¢User-Agent
            self.headers['User-Agent'] = self.get_random_user_agent()

            # æ·»åŠ éšæœºå»¶æ—¶
            time.sleep(random.uniform(1, 3))

            response = requests.get(self.url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                return response.json()
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
                return None

        except Exception as e:
            print(f"è·å–çƒ­æœå¤±è´¥: {str(e)}")
            return None

    def parse_hot_search(self, data):
        """è§£æçƒ­æœæ•°æ®"""
        try:
            hot_searches = []

            if 'data' in data and 'realtime' in data['data']:
                for item in data['data']['realtime']:
                    hot_item = {
                        'rank': item.get('rank', ''),
                        'topic': item.get('note', ''),
                        'hot_score': item.get('raw_hot', 0),
                        'category': item.get('category', ''),
                        'url': f"https://s.weibo.com/weibo?q=%23{item.get('note', '')}%23",
                        'is_hot': item.get('is_hot', False),
                        'is_boom': item.get('is_boom', False),
                        'is_new': item.get('is_new', False)
                    }
                    hot_searches.append(hot_item)

            return hot_searches

        except Exception as e:
            print(f"è§£ææ•°æ®å¤±è´¥: {str(e)}")
            return []

    def save_to_json(self, hot_searches):
        """ä¿å­˜çƒ­æœæ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'weibo_hot_search_{timestamp}.json'

            # æ·»åŠ å…ƒæ•°æ®
            data = {
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'weibo.com',
                'total_count': len(hot_searches),
                'hot_searches': hot_searches
            }

            # å°†æ•°æ®å†™å…¥JSONæ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
            return True

        except Exception as e:
            print(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    def crawl(self):
        """æ‰§è¡Œçˆ¬è™«ä¸»æµç¨‹"""
        print("å¼€å§‹çˆ¬å–å¾®åšçƒ­æœæ¦œ...")

        # è·å–çƒ­æœæ•°æ®
        raw_data = self.fetch_hot_search()
        if not raw_data:
            return False

        # è§£ææ•°æ®
        hot_searches = self.parse_hot_search(raw_data)
        if not hot_searches:
            print("æœªæ‰¾åˆ°çƒ­æœæ•°æ®")
            return False

        # æ‰“å°çˆ¬å–ç»“æœ
        print(f"\næˆåŠŸçˆ¬å– {len(hot_searches)} æ¡çƒ­æœ:")
        for item in hot_searches:
            print(f"\n{item['rank']}. {item['topic']}")
            print(f"çƒ­åº¦: {item['hot_score']}")
            print(f"åˆ†ç±»: {item['category']}")
            if item['is_hot']:
                print("ğŸ”¥ çƒ­é—¨")
            if item['is_boom']:
                print("ğŸ’¥ çˆ†")
            if item['is_new']:
                print("ğŸ†• æ–°")

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        return self.save_to_json(hot_searches)


def main():
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WeiboHotSearchCrawler()

    try:
        # æ‰§è¡Œçˆ¬è™«
        success = crawler.crawl()

        if success:
            print("\nçˆ¬å–å®Œæˆ!")
        else:
            print("\nçˆ¬å–å¤±è´¥!")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {str(e)}")


if __name__ == "__main__":
    main()